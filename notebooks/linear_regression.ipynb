{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11894e51-9944-41a9-8bf8-4b93398436bb",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1953b03-9769-4ef1-a019-1e104999ae76",
   "metadata": {},
   "source": [
    "**Linear regression** is a model that estimates the relationship between a scalar response (dependent variable) and one or more explanatory variables (regressor or independent variable). The relationship is modeled using a linear predictor function, whose unknown parameters are estimated from the data.\n",
    "\n",
    "In a **machine learning** context, linear regression is a **supervised algorithm** that learns from labeled training data by fitting the best possible linear function to the input features. Once trained, this function can be used to make predictions on new, unseen data.\n",
    "\n",
    "The linear predictor function can have the following forms:\n",
    "\n",
    "- One predictor:\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x\n",
    "$$\n",
    "\n",
    "- Multiple predictors:\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n \\\\\n",
    "$$\n",
    "or\n",
    "$$\n",
    "\\hat{y} = \\beta_0 + \\sum_{i=1}^{n} \\beta_i x_i\n",
    "$$\n",
    "\n",
    "- Matrix form:\n",
    "$$\n",
    "\\hat{y} = X \\boldsymbol{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662fe180-71f6-4fa9-b542-4286a1c87a7d",
   "metadata": {},
   "source": [
    "## Uses\n",
    "Most applications of linear regression fall into one of the two categories:\n",
    "\n",
    "- **Prediction**: Fitting a model to observed data in order to predict the response variable using only the explanatory (independent) variables. Once trained, the model can generate predictions for new, unseen data.\n",
    "\n",
    "- **Interpretation**: Understanding and quantifying how much of the variation in the response variable can be explained by changes in the explanatory variables. This is useful for identifying significant relationships and trends in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a6f01-786d-4bfb-990c-b67f01735b8b",
   "metadata": {},
   "source": [
    "## Fitting\n",
    "\n",
    "Linear regression models are often fitted using a method called **Ordinary Least Squares (OLS)**. The goal of OLS is to find the parameter values that minimize the difference between the actual and predicted values of the response variable.\n",
    "\n",
    "This difference is quantified using the **Mean Squared Error (MSE)**, which is the average of the squared differences between actual values ($y_i$) and predicted values ($\\hat{y}_i$). This is also known as the loss function:\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^{n}(y_i-\\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "> **Note:** Some sources include the $\\frac{1}{n}$ term when defining Mean Squared Error (MSE), while others omit it in the context of model fitting. This difference does not affect the estimated coefficients — since scaling the loss function by a constant doesn't change the location of its minimum.\n",
    "\n",
    "To minimize this error, OLS solves for the parameters, $\\boldsymbol{\\beta}$, using the following closed-form solution:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\beta} = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $X$: the feature matrix (with a column of ones for the intercept)\n",
    "- $y$: the target vector\n",
    "- $\\boldsymbol{\\beta}$: the vector of fitted coefficients\n",
    "\n",
    "Once fitted, the model can predict outcomes using:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X \\boldsymbol{\\beta}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953cf300-2bc4-4107-aa2f-c3d6118c9124",
   "metadata": {},
   "source": [
    "## Error and Residuals\n",
    "\n",
    "In linear regression, understanding the distinction between **error** and **residual** is key.\n",
    "\n",
    "- The **error term** $\\varepsilon_i$ represents the true, unobservable deviation of the observed response $y_i$ from the true regression line. It captures effects of omitted variables, measurement noise, and randomness:\n",
    "\n",
    "$$\n",
    "y_i = \\hat{y}_i + \\varepsilon_i\n",
    "$$\n",
    "\n",
    "- The **residual** $e_i$ is the observed counterpart to the error. It is the difference between the actual value and the predicted value from the model:\n",
    "\n",
    "$$\n",
    "e_i = y_i - \\hat{y}_i\n",
    "$$\n",
    "\n",
    "While the true error $\\varepsilon_i$ is unknown, the residual $e_i$ is used to assess model performance and validate regression assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8921f-d1f5-4161-b720-cd0c0a30359a",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "Standard linear regression models with standard estimation techniques make a number of assumptions about the predictors, the response, and their relationship. These assumptions ensure that the estimated coefficients $\\boldsymbol{\\beta}$ are unbiased and that inference (confidence intervals and p-values) is valid.\n",
    "\n",
    "- **Weak exogeneity**: The predictors are not correlated with the error term $\\varepsilon$. This ensures that the model is not influenced by omitted variable bias.\n",
    "\n",
    "- **Linearity**: The relationship between the predictors and the response is linear in the parameters. That is, the expected value of the response variable is a linear combination of the predictors.\n",
    "\n",
    "- **Constant variance (homoscedasticity)**: The variance of the error term is constant across all values of the independent variables. In other words, the spread of residuals should be uniform.\n",
    "\n",
    "- **Independence of errors**: The residuals should be independent of each other -- no autocorrelation.\n",
    "\n",
    "- **Lack of perfect multicollinearity**: The independent variables should not be perfectly linearly related to each other. Perfect multicollinearity makes it impossible to estimate unique coefficients.\n",
    "\n",
    "Violations of these assumptions can result in biased estimations of $\\boldsymbol{\\beta}$, biased standard errors, untrustworthy confidence intervals and significance tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cef496-bfea-4327-b069-a58a37522e3f",
   "metadata": {},
   "source": [
    "## Assessing Assumptions\n",
    "\n",
    "Once a linear regression model is fitted, it's important to verify that the key assumptions are reasonably satisfied. Below are common techniques for diagnosing each assumption.\n",
    "\n",
    "**Linearity**: Check whether the relationship between the predictors and response is linear.\n",
    "- Plot residuals vs. fitted values. A random scatter indicates linearity while patterns indicate nonlinearity.\n",
    "- Use partial regression plots to check linearity with individual predictors.\n",
    "\n",
    "**Homoscedasticity**: Ensure the spread of residuals is constant across fitted values.\n",
    "- Plot residuals vs. fitted values.\n",
    "- Look for \"funnel\" or \"bowtie\" patterns.\n",
    "- Breusch-Pagan test.\n",
    "\n",
    "**Independence of Errors**: Ensure residuals are independent (no autocorrelation).\n",
    "- Durbin-Watson test for time series or ordered data.\n",
    "- Plot residuals over time or observation order\n",
    "\n",
    "**Normality of Errors**: Residuals should be roughly normally distributed for valid confidence intervals.\n",
    "- Plot a histogram or Q-Q plot of residuals.\n",
    "- Shapiro-Wilk or Kolmogorov-Smirnov tests for normality.\n",
    "\n",
    "**No Perfect Multicollinearity**: Ensure no predictors are perfectly (or near perfectly) linearly dependent.\n",
    "- Calculate the **Variance Inflation Factor (VIF)** for each feature\n",
    "- VIF > 5-10 indicates multicollinearity.\n",
    "\n",
    "While mild violations are common and often tolerable, significant deviations from these assumptions can lead to biased estimates or invalid inference. These checks help decide when model adjustments are necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772d094-581d-4ff6-8bf2-ffc0dac5c23d",
   "metadata": {},
   "source": [
    "## Overfitting and Regularization\n",
    "\n",
    "A linear model may perform well on training data but poorly on unseen data — this is called **overfitting**. It often happens when the model is too complex or the data is noisy.\n",
    "\n",
    "To address this, we use **regularization**, which adds a penalty to large coefficients. This encourages simpler models that generalize better.\n",
    "\n",
    "- **L2 (Ridge)**: Shrinks coefficients but keeps all features\n",
    "- **L1 (Lasso)**: Can shrink some coefficients to zero, effectively performing **feature selection**\n",
    "\n",
    "These two regularization methods are alternatives to the MSE loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006d9c7-096b-4941-a477-38dde33f2491",
   "metadata": {},
   "source": [
    "## Regularized Loss Functions\n",
    "\n",
    "Regularization modifies the original loss function (Mean Squared Error) by adding a penalty term.\n",
    "\n",
    "- **Ordinary Least Squares (OLS)** minimizes:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$\n",
    "\n",
    "- **Ridge Regression (L2 penalty)** adds a squared penalty on coefficients:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "- **Lasso Regression (L1 penalty)** adds an absolute value penalty:\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "- $\\lambda$ controls the strength of regularization. Higher values shrink coefficients more aggressively.\n",
    "\n",
    "In practice, $\\lambda$ is often selected using cross-validation.\n",
    "\n",
    "> **Note:** In most implementations (including scikit-learn), regularization is applied **only to the coefficients**, not the intercept $\\beta_0$. This ensures that the model can still fit the baseline level of the response variable without penalty.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af8f7fb-84ae-4e89-b805-f93a11803739",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c8a5e-6514-467f-9c1a-c7f8bdf3c9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
