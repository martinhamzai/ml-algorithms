{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1871ece-01da-4aa2-beab-7e56a89374a6",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "**Random forests** is an ensemble learning method for classification and regression that works by creating a multitude of decision trees. For classification, the output of the random forest is the class selected by most trees. For regression tasks, the output is the average of all of the predictions of the trees. Random forests correct for decision trees' habit of overfitting. They are a supervised learning method and an ensemble learning method, meaning it combines multiple learning algorithms to improve predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46b22ee-cc31-4772-99ad-71aaabd6e99c",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "### Preliminary: Decision Tree Learning\n",
    "\n",
    "Random Forests are composed of multiple decision trees trained on various subsets of the data. The goal of random forests is to reduce the variance and shortcomings of a singular decision tree by averaging multiple deep decision trees. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the model.\n",
    "\n",
    "### Bagging\n",
    "The training algorithm for a random forest applies the general technique of boostrap aggregating, or bagging, to tree learners.\n",
    "\n",
    "#### General Technique\n",
    "Given a standard training dataset $D$ of size $n$, bagging generates $m$ new training datasets $D_i$ of size $n'$, by sampling from $D$ uniformly and with replacement. Sampling with replacement may result in some observations being repeated across different training datasets, but ensures that each bootstrap is independent from its peers. Then, $m$ models are fitting using the bootstrap samples and combined by averaging the output (regression) or voting (classification).\n",
    "\n",
    "For regression, the prediction is:\n",
    "$$\n",
    "\\hat{f} = \\frac{1}{B}\\sum_{b=1}^{B}f_b(x')\n",
    "$$\n",
    "\n",
    "For classification, the prediction is the plurality vote.\n",
    "\n",
    "This bagging technique leads to better model performance because it decreases the variance of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated, which is the case if trees are training on the same training set. Bootstrap sampling is a way of de-correlating the trees by showing them different training sets.\n",
    "\n",
    "Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on $x'$:\n",
    "$$\n",
    "\\sigma = \\sqrt{\\frac{\\sum_{b=1}^B(f_b(x')-\\hat{f})^2}{B-1}}\n",
    "$$\n",
    "\n",
    "The number of trees/samples $B$ is a free parameter. $B$ can be optimized by using cross-validation or by observing the out-of-bag error.\n",
    "\n",
    "### Feature Bagging\n",
    "Another type of bagging scheme that random forests use is called feature bagging. Feature bagging utilizes a modified tree learning algorithm that selects, at each candidate split in the learning process, a random subset of the features. The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or few features are very strong predictors for the response, these features will be selected in many of the $B$ trees, causing them to become correlated.\n",
    "\n",
    "Typically, for a classification problem with $p$ features, $\\sqrt{p}$ (rounded down) features are used in each split. For regression problems, $p/3$ (rounded down) with a minimum node size of 5 as the default. In practice, the best values for these parameters should be handled on a case-to-case basis.\n",
    "\n",
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450b8e20-c710-4e76-bf08-d821dac7febf",
   "metadata": {},
   "source": [
    "## Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178bc60c-bbe4-450f-aa4c-69e8c3b13a97",
   "metadata": {},
   "source": [
    "## When to Use, Not to Use, Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439295c5-529f-4633-93d0-3d9b99e4a13a",
   "metadata": {},
   "source": [
    "# Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2783287e-8b32-429c-93aa-8110b8a73844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
